{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffea4a3d-51f0-4b1f-9650-77434f1aac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"personas.txt\",\"r\") as fp:\n",
    "    data = fp.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "492a38a8-7d30-486d-b09a-3171b25872b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.split(\"\\n\\n\\n\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcbd31b7-352d-4a03-8a80-d4a80fa18d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b6afb5f-d11f-4194-b1a7-99671e87d00b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prompts:\\n  prompt: &prompt |-\\n    [Question]\\n    ${source_text}\\n    [The Start of Assistant 1’s Answer]\\n    ${compared_text_one}\\n    [The End of Assistant 1’s Answer]\\n    [The Start of Assistant 2’s Answer]\\n    ${compared_text_two}\\n    [The End of Assistant 2’s Answer]\\n    [System]\\n    We would like to request your feedback on the performance of two AI assistants in response to the user question displayed above.\\n    Please consider the helpfulness, relevance, accuracy, and level of detail of their responses.\\n    There are a few other referee assigned the same task, it\\'s your responsibility to discuss with them and think critically before you make your final judgement.\\n    Each assistant receives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance.\\n\\n    Here is your discuss history:\\n    ${chat_history}\\n\\n    ${role_description}\\n\\n    Now it\\'s your time to talk, please make your talk short and clear, ${agent_name} !\\n\\n    ${final_prompt}\\n\\n\\nenvironment:\\n  env_type: llm_eval\\n  max_turns: 4\\n  rule:\\n    order:\\n      type: sequential\\n    visibility:\\n      type: all\\n    selector:\\n      type: basic\\n    updater:\\n      type: basic\\n    describer:\\n      type: basic\\n\\nagents:\\n  -\\n    agent_type: llm_eval_multi\\n    name: Urban Planner\\n    final_prompt_to_use: |-\\n      Please first provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment.\\n      Then, output two lines indicating the scores for Assistant 1 and 2, respectively.\\n\\n      Remember that you are not required to output the same value as other referees !\\n      Output with the following format strictly:\\n      Evaluation evidence: [your explanation here]\\n      The score of Assistant 1: [score only]\\n      The score of Assistant 2: [score only]\\n    role_description: |-\\n       You are the Urban Planner, one of the referees in this task. You specialize in designing and developing urban areas, including transportation systems. Your expertise lies in creating inclusive and accessible public spaces. Please help others determine which response is better.\\n    memory:\\n      memory_type: chat_history\\n    memory_manipulator:\\n      memory_manipulator_type: basic\\n    prompt_template: *prompt\\n    llm:\\n      model: \"gpt-3.5-turbo-0301\"\\n      llm_type: gpt-3.5-turbo-0301\\n      temperature: 0\\n      max_tokens: 512\\n  -\\n    agent_type: llm_eval_multi\\n    name: Accessibility Advocate\\n    final_prompt_to_use: |-\\n      Please first provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment.\\n      Then, output two lines indicating the scores for Assistant 1 and 2, respectively.\\n\\n      Remember that you are not required to output the same value as other referees !\\n      Output with the following format strictly:\\n      Evaluation evidence: [your explanation here]\\n      The score of Assistant 1: [score only]\\n      The score of Assistant 2: [score only]\\n    role_description: |-\\n      You are the Accessibility Advocate, one of the referees in this task. You have extensive knowledge and experience in advocating for the rights of individuals with disabilities. Your role is to ensure that the responses consider the needs and requirements of all individuals, regardless of their abilities. Please help others determine which response is better.\\n    memory:\\n      memory_type: chat_history\\n    memory_manipulator:\\n      memory_manipulator_type: basic\\n    prompt_template: *prompt\\n    llm:\\n      model: \"gpt-3.5-turbo-0301\"\\n      llm_type: gpt-3.5-turbo-0301\\n      temperature: 0\\n      max_tokens: 512\\n\\ntools: ~'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc1a6729-39b9-4cfa-8366-7f7f76054f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = re.match(r\"role_description: \\|-\\n      ([\\d\\D]+?)\\n   \",config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "633589a3-8f8d-434e-9091-8a8d6e7e4be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config_test.yaml', 'w') as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bfd183fc-2a49-4281-a1fe-f636ccc9375f",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = re.finditer(r\"role_description: \\|-\\n([\\d\\D]+?)\\n\", config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "daa6fad1-d9f8-46ed-861d-37ab0d130f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "regex = r\"role_description: \\|-\\n([\\d\\D]+?)\\n \"\n",
    "\n",
    "\n",
    "\n",
    "matches = re.finditer(regex, config)\n",
    "modified_str = config\n",
    "\n",
    "for matchNum, match in enumerate(matches, start=1):\n",
    "    for groupNum in range(0, len(match.groups())):\n",
    "        groupNum = groupNum + 1\n",
    "        \n",
    "        if matchNum == 1 and groupNum == 1:\n",
    "            modified_str = modified_str.replace(match.group(groupNum), \"      test 1\", 1)\n",
    "        elif matchNum == 2 and groupNum == 1:\n",
    "            modified_str = modified_str.replace(match.group(groupNum), \"      test 2\", 1)\n",
    "\n",
    "# print(modified_str)\n",
    "with open('config_test.yaml', 'w') as f:\n",
    "    f.write(modified_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6824d21-fc1e-489e-87cc-b34e78345fbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
